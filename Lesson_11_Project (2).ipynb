{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pathtogs='C:\\\\Program Files\\\\gs\\\\gs9.24\\\\bin'\n",
    "os.environ['PATH']+=os.pathsep+pathtogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write separate python functions which accept a string and a number ‘n’ and return the following:\n",
    "\n",
    "a.\tN most frequent Nouns  \n",
    "Take function name as “GetNMostFrequentNouns”\n",
    "\n",
    "b.\tN most frequent Verbs\n",
    "Take function name as “GetNMostFrequentVerbs”\n",
    "\n",
    "c.\tN most frequent Delimiters\n",
    "Take function name as “GetNMostFrequentDelimiters”\n",
    "\n",
    "d.\tN most frequent Prepositions\n",
    "Take function name as “GetNMostFrequentPrepositions”\n",
    "\n",
    "** These functions will help you to in last question of this case study.\n",
    "Run all the function on the file “FIFAWorldCup2018.docx” and print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def GetPOSTags(string):\n",
    "    tokens=nltk.word_tokenize(string)\n",
    "    pos_tags=nltk.pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "def GetNMostFrequentNouns(string,n):\n",
    "    POSTags=GetPOSTags(string)\n",
    "    allForms=['NN','NNP','NNS','NNPS']\n",
    "    count=0\n",
    "    words=[]\n",
    "    for word,tag in POSTags:\n",
    "        if tag in allForms:\n",
    "            words.append(word.lower())\n",
    "    c=Counter(words)\n",
    "    return list(dict(c.most_common(n)).keys())\n",
    "\n",
    "def GetNMostFrequentVerbs(string,n):\n",
    "    POSTags=GetPOSTags(string)\n",
    "    allForms=['VB','VBP','VBZ','VBG','VBD','VBN']\n",
    "    count=0\n",
    "    words=[]\n",
    "    for word,tag in POSTags:\n",
    "        if tag in allForms:\n",
    "            words.append(word.lower())\n",
    "    c=Counter(words)\n",
    "    return list(dict(c.most_common(n)).keys())\n",
    "\n",
    "def GetNMostFrequentDelimiters(string,n):\n",
    "    POSTags=GetPOSTags(string)\n",
    "    allForms=['DT']\n",
    "    count=0\n",
    "    words=[]\n",
    "    for word,tag in POSTags:\n",
    "        if tag in allForms:\n",
    "            words.append(word.lower())\n",
    "    c=Counter(words)\n",
    "    return list(dict(c.most_common(n)).keys())\n",
    "\n",
    "\n",
    "def GetNMostFrequentPrepositions(string,n):\n",
    "    POSTags=GetPOSTags(string)\n",
    "    allForms=['IN']\n",
    "    count=0\n",
    "    words=[]\n",
    "    for word,tag in POSTags:\n",
    "        if tag in allForms:\n",
    "            words.append(word.lower())\n",
    "    c=Counter(words)\n",
    "    return list(dict(c.most_common(n)).keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all the function on the file “FIFAWorldCup2018.docx” and print the results.\n",
    "\n",
    "#reading the file\n",
    "file=open(\"FIFAWorldCup2018.txt\",'r')\n",
    "content=file.read()\n",
    "file.close()\n",
    "\n",
    "#5 most frequent noun\n",
    "print (\"5 Most Frequent Nouns:\",GetNMostFrequentNouns(content,5))\n",
    "\n",
    "\n",
    "#5 most frequent verbs\n",
    "print (\"5 Most Frequent Verbs:\",GetNMostFrequentVerbs(content,5))\n",
    "\n",
    "\n",
    "\n",
    "#4 most frequent delimiters\n",
    "print (\"4 Most Frequent Nouns:\",GetNMostFrequentDelimiters(content,4))\n",
    "\n",
    "\n",
    "#5 most frequent prepositions\n",
    "print (\"5 Most Frequent Perpositions:\",GetNMostFrequentPrepositions(content,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a python function which accepts a string and prints the first  sentence in the string along with its syntax tree.\n",
    "take function name as “PrintSyntaxTrees”.\n",
    "Run all the functions on the file “FIFAWorldCup2018.txt” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def PrintSyntaxTree(sent):\n",
    "    first_sent=sent.split(\".\")[0]+\".\"\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(first_sent))\n",
    "    grammar=r\"NP:{<DT>?<JJ>*<NN>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "    print (first_sent)\n",
    "    return chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this function on the file “FIFAWorldCup2018.txt” \n",
    "\n",
    "#Reading the file\n",
    "file=open(\"FIFAWorldCup2018.txt\",'r')\n",
    "content=file.read()\n",
    "file.close()\n",
    "\n",
    "#Printing the tree \n",
    "Print(SyntaxTree(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write python functions which accept a string and return the following using regular expressions: \n",
    "\n",
    "a.\tText from the string after removing all the punctuations \n",
    "take function name as “TextAfterRemovingPunctuations”\n",
    "\n",
    "b.\tText from the string after removing all the numbers/digits \n",
    "take function name as “TextAfterRemovingDigits”\n",
    "\n",
    "c.\tAll the words which begin with the capital letter \n",
    "take function name as “AllCapitalizedWordsFromText”\n",
    "\n",
    "d.\tAll the emails from the string\n",
    "take function name as “AllEmailsFromText.”\n",
    "\n",
    "Run all the above functions on the file “FIFAWorldCup2018.txt” and print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def TextAfterRemovingPunctuations(string):\n",
    "    s = re.sub(r'[^\\w\\s]','',string)\n",
    "    return s\n",
    "\n",
    "def TextAfterRemovingDigits(string):\n",
    "    s=re.sub(r\"(^|\\W)\\d+\", \"\", string)\n",
    "    return s\n",
    "\n",
    "def AllCapitalizedWordsFromText(string):\n",
    "    s = re.findall(r'(\\b[A-Z]([a-z])*\\b)',string)\n",
    "    Capitalized=[]\n",
    "    for word,lastcharacter in s:\n",
    "        Capitalized.append(word)\n",
    "    return Capitalized\n",
    "    \n",
    "def AllEmailsFromText(string):\n",
    "    match = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', string)\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all the above functions on the file “FIFAWorldCup2018.txt” and print the results.\n",
    "\n",
    "#Reading the file\n",
    "file=open(\"FIFAWorldCup2018.txt\",'r')\n",
    "content=file.read()\n",
    "file.close()\n",
    "\n",
    "print (\"After Removing Punctuation\")\n",
    "print (TextAfterRemovingPunctuations(content))\n",
    "print (\"\\n\")\n",
    "\n",
    "print (\"After Removing Digits\")\n",
    "print (TextAfterRemovingDigits(content))\n",
    "print (\"\\n\")\n",
    "\n",
    "print (\"Capitalized Words\")\n",
    "print (AllCapitalizedWordsFromText(content))\n",
    "print (\"\\n\")\n",
    "\n",
    "print (\"Emails\")\n",
    "print (AllEmailsFromText(content))\n",
    "print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.\tWrite python functions which accept a string as an input and return the following chunks.\n",
    "\n",
    "a.\tPhrases having Proper nouns followed by Verbs\n",
    "take function name as “ChunkingVer1”\n",
    "\n",
    "b.\tVerb Phrases having Verbs followed by Adjectives\n",
    "take function name as “ChunkingVer2”\n",
    "\n",
    "c.\tNoun Phrases having Determiners followed by Nouns\n",
    "take function name as “ChunkingVer3”\n",
    "\n",
    "d.\tVerb Phrases having Verbs followed by Adverbs\n",
    "Take function name as “ChunkingVer4”\n",
    "\n",
    "e.\tPhrases having Delimiter, Adjectives and Nouns in the respective order\n",
    "take function name as “ChunkingVer5”\n",
    "\n",
    "f.\tNoun Phrases having Nouns and Adjectives, terminated with Nouns\n",
    "take function name as “ChunkingVer6.”\n",
    "\n",
    "Run all the functions for the first sentence in the file “FIFAWorldCup2018.txt” and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#Phrases having Proper nouns followed by Verbs\n",
    "def ChunkingVer1(sent):\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    grammar=r\"Phrases:{<NNP|NNPS>*<NNP|NNPS><VB|VBD|VBG|VBZ|VBP|VBN>*<VB|VBD|VBG|VBZ|VBP|VBN>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "    return chunk_result\n",
    "\n",
    "\n",
    "#Verb Phrases having Verbs followed by Adjectives\n",
    "def ChunkingVer2(sent):\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    grammar=r\"VerbPhrases:{<VB|VBD|VBG|VBZ|VBP|VBN>*<VB|VBD|VBG|VBZ|VBP|VBN><JJ|JJR|JJS>*<JJ|JJR|JJS>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "    return chunk_result\n",
    "\n",
    "#Noun Phrases having Determiners followed by Nouns\n",
    "def ChunkingVer3(sent):\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    grammar=r\"NounPhrases:{<DT><NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "    return chunk_result\n",
    "\n",
    "#Verb Phrases having Verbs followed by Adverbs\n",
    "def ChunkingVer4(sent):\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    grammar=r\"VerbPhrases:{<VB|VBD|VBG|VBZ|VBP|VBN>*<VB|VBD|VBG|VBZ|VBP|VBN><RB|RBR>*<RB|RBR>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "    return chunk_result\n",
    "\n",
    "#Phrases having Delimiter, Adjectives and Nouns in the respective order.\n",
    "def ChunkingVer5(sent):\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    grammar=r\"Phrases:{<DT><JJ><NN|NNP|NNS|NNPS>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "    return chunk_result\n",
    "\n",
    "#Noun Phrases which having Nouns and Adjectives, terminated with Nouns. \n",
    "def ChunkingVer6(sent):\n",
    "    sent_tokens=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    grammar=r\"NounPhrases:{<NN|NNP|NNS|NNPS><JJ><NN|NNP|NNS|NNPS>}\"\n",
    "    chunk_parser=nltk.RegexpParser(grammar)\n",
    "    chunk_result=chunk_parser.parse(sent_tokens)\n",
    "\n",
    "    return chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all the function for the first sentence in the file “FIFAWorldCup2018.txt” and print the results.\n",
    "\n",
    "file=open(\"FIFAWorldCup2018.txt\",'r')\n",
    "content=file.read()\n",
    "file.close()\n",
    "\n",
    "first_sent=content.split(\".\")[0]+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkingVer1(first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkingVer2(first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkingVer3(first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkingVer4(first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkingVer5(first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkingVer6(first_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5.\tMake a content free grammar having the following rules: \n",
    "\n",
    "a.\tNoun Phrases are followed by Verb Phrases\n",
    "\n",
    "b.\tVerb Phrase can have \n",
    "i.\tVerb and noun phrase \n",
    "ii.\tVerb, noun phrase and a preposition\n",
    "\n",
    "c.\tNoun Phrases can have \n",
    "i.\tDelimiters followed by noun \n",
    "ii.\tDelimiters, noun, and a preposition phrase,\n",
    "\n",
    "d.\tPreposition phrase have a preposition followed by a noun phrase\n",
    "e. The delimiters, verbs, prepositions and nouns for the grammar should be the two most frequent words of each type from “FIFAWorldCup2018.txt”.\n",
    " Generate the CFG for “FIFAWorldCup2018.txt” file and save them in a file named “CFG.txt”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 most common Nouns\n",
    "print (\"Nouns:\",GetNMostFrequentNouns(content,2))\n",
    "\n",
    "#2 most common verbs\n",
    "print (\"Verbs:\",GetNMostFrequentVerbs(content,2))\n",
    "\n",
    "#2 most common prepositions\n",
    "print (\"Prepositions:\",GetNMostFrequentPrepositions(content,2))\n",
    "\n",
    "#2 most common delimiters\n",
    "print (\"Delimiters:\",GetNMostFrequentDelimiters(content,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " VP -> V NP | V NP PP\n",
    " V -> \"had\" | \"was\"\n",
    " NP -> DT N \n",
    " DT -> \"a\" | \"the\" \n",
    " N -> \"world\" | \"cup\"\n",
    " PP -> P NP\n",
    " P -> \"of\" | \"in\"\n",
    " \"\"\")\n",
    "\n",
    "generated_grammar=[]\n",
    "for sent in generate(grammar):\n",
    "    generated_grammar.append(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving in a file name \"CGF.txt\"\n",
    "\n",
    "file=open('CFG.txt','a+')\n",
    "for each in generated_grammar:\n",
    "    file.write(each+\"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
